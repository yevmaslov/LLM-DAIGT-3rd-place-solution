{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize\n",
    "import nltk \n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('data/train_dataset_v1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nWhen considering the pros and cons of attend...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nBefore making any decisions about getting in...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nRalph Waldo Emerson once said, \"Go confident...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nHuman character traits are shaped by a wide ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nOnline classes have been increasingly popula...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11044</th>\n",
       "      <td>“In the old tunnel that runs from this place n...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11045</th>\n",
       "      <td>'I have thought of something, dearest. Do as y...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11046</th>\n",
       "      <td>“‘That’s my brave girl. It’s better worth bein...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11047</th>\n",
       "      <td>“There’s something ever egotistical in mountai...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11048</th>\n",
       "      <td>“No—you see, he went for morality: I don’t car...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11049 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  generated\n",
       "0      \\nWhen considering the pros and cons of attend...        1.0\n",
       "1      \\nBefore making any decisions about getting in...        1.0\n",
       "2      \\nRalph Waldo Emerson once said, \"Go confident...        1.0\n",
       "3      \\nHuman character traits are shaped by a wide ...        1.0\n",
       "4      \\nOnline classes have been increasingly popula...        1.0\n",
       "...                                                  ...        ...\n",
       "11044  “In the old tunnel that runs from this place n...        0.0\n",
       "11045  'I have thought of something, dearest. Do as y...        0.0\n",
       "11046  “‘That’s my brave girl. It’s better worth bein...        0.0\n",
       "11047  “There’s something ever egotistical in mountai...        0.0\n",
       "11048  “No—you see, he went for morality: I don’t car...        0.0\n",
       "\n",
       "[11049 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/slimpajama.parquet')\n",
    "\n",
    "model = 'upstage/SOLAR-10.7B-Instruct-v1.0'\n",
    "output_folder = f'data/generated/{model}'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "folders = os.listdir('data/generated/')\n",
    "folders = [f for f in folders if 'pajama_gen' in f and '.' not in f]\n",
    "\n",
    "generated_ids = {}\n",
    "for folder in folders:\n",
    "    fns = os.listdir(folder)\n",
    "    fns = [fn for fn in fns if fn.startswith('text_')]\n",
    "    for fn in fns:\n",
    "        fn = fn.split('_')[1].split('.')[0]\n",
    "        generated_ids[fn] = True\n",
    "        \n",
    "df['already_generated'] = df['generated_id'].apply(lambda x: generated_ids.get(x, False))\n",
    "df = df[~df['already_generated']]\n",
    "\n",
    "remaining_indexes = df.generated_id.values\n",
    "np.random.shuffle(remaining_indexes)\n",
    "\n",
    "print(len(remaining_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = LLM(model=model, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in tqdm(range(0, len(remaining_indexes), 128)):\n",
    "    prompts = []\n",
    "    ids = []\n",
    "    texts = []\n",
    "    for j in range(i, i+128):\n",
    "        id = remaining_indexes[j]\n",
    "        row = df[df['generated_id'] == id]\n",
    "        if os.path.isfile(f'{output_folder}/text_{id}.txt'):\n",
    "            continue\n",
    "\n",
    "        text = row.text.values[0]\n",
    "        text_len = row.text_len.values[0]\n",
    "        min_text_len = 60\n",
    "        max_text_len = 150\n",
    "\n",
    "        text = ' '.join(text.split(' ')[:np.random.randint(min_text_len, max_text_len)])\n",
    "        prompt = f'Continue following text, generate response that is 500-600 words long: {text}'\n",
    "\n",
    "        prompts.append(prompt)\n",
    "        ids.append(id)\n",
    "        texts.append(text)\n",
    "        \n",
    "\n",
    "    max_tokens = [600, 700, 800, 900,]\n",
    "    \n",
    "    mid_temp_range = (0.8, 1)\n",
    "    high_temp_range = (1.2, 1.5)\n",
    "    \n",
    "    p_range = (0.8, 0.95)\n",
    "    \n",
    "    mid_mp_range = (0.01, 0.02)\n",
    "    high_mp_range = (0.05, 0.07)\n",
    "    \n",
    "    repetition_range = (1, 1.05)\n",
    "    frequency_range = (0, 0.1)\n",
    "    presence_range = (0, 1)\n",
    "\n",
    "    sampling_params = np.random.choice([\n",
    "        SamplingParams(\n",
    "            temperature=np.random.uniform(mid_temp_range[0], mid_temp_range[1]), \n",
    "            top_p=np.random.uniform(p_range[0], p_range[1]), \n",
    "            max_tokens=np.random.choice(max_tokens),\n",
    "            repetition_penalty=np.random.uniform(repetition_range[0], repetition_range[1]), \n",
    "            presence_penalty=np.random.uniform(presence_range[0], presence_range[1])\n",
    "        ),\n",
    "        SamplingParams(\n",
    "            temperature=np.random.uniform(mid_temp_range[0], mid_temp_range[1]), \n",
    "            top_p=np.random.uniform(p_range[0], p_range[1]), \n",
    "            max_tokens=np.random.choice(max_tokens),\n",
    "            frequency_penalty=np.random.uniform(frequency_range[0], frequency_range[1]), \n",
    "            presence_penalty=np.random.uniform(presence_range[0], presence_range[1])\n",
    "        ),\n",
    "        \n",
    "        SamplingParams(\n",
    "            temperature=np.random.uniform(high_temp_range[0], high_temp_range[1]), \n",
    "            min_p=np.random.uniform(high_mp_range[0], high_mp_range[1]),\n",
    "            max_tokens=np.random.choice(max_tokens),\n",
    "            repetition_penalty=np.random.uniform(repetition_range[0], repetition_range[1]), \n",
    "            presence_penalty=np.random.uniform(presence_range[0], presence_range[1])\n",
    "        ),\n",
    "        SamplingParams(\n",
    "            temperature=np.random.uniform(high_temp_range[0], high_temp_range[1]), \n",
    "            min_p=np.random.uniform(high_mp_range[0], high_mp_range[1]),\n",
    "            max_tokens=np.random.choice(max_tokens),\n",
    "            frequency_penalty=np.random.uniform(frequency_range[0], frequency_range[1]), \n",
    "            presence_penalty=np.random.uniform(presence_range[0], presence_range[1])\n",
    "        ),\n",
    "    ])\n",
    "        \n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    for id, output, text in zip(ids, outputs, texts):\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        \n",
    "        new_text = text.strip() + ' ' + generated_text.strip()\n",
    "        new_text = new_text.replace('### Answer:', '').replace('###Answer:', '')\n",
    "        \n",
    "        with open(f'{output_folder}/text_{id}.txt', 'w') as file:\n",
    "            file.write(new_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
